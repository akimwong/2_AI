{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28f900e-9a65-4338-96bb-b1355a7b7d74",
   "metadata": {},
   "source": [
    "## LangGraph - The Easy Way\n",
    "https://www.youtube.com/watch?v=R8KB-Zcynxc&t=236s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "c3994972-98a3-42be-b231-80e264f5c157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: python-dotenv\n",
      "Version: 0.21.0\n",
      "Summary: Read key-value pairs from a .env file and set them as environment variables\n",
      "Home-page: https://github.com/theskumar/python-dotenv\n",
      "Author: Saurabh Kumar\n",
      "Author-email: me+github@saurabh-kumar.com\n",
      "License: BSD-3-Clause\n",
      "Location: C:\\Users\\CGM\\anaconda3\\Lib\\site-packages\n",
      "Requires: \n",
      "Required-by: anaconda-cloud-auth, pydantic-settings\n",
      "---\n",
      "Name: langgraph\n",
      "Version: 0.2.59\n",
      "Summary: Building stateful, multi-actor applications with LLMs\n",
      "Home-page: https://www.github.com/langchain-ai/langgraph\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\CGM\\anaconda3\\Lib\\site-packages\n",
      "Requires: langchain-core, langgraph-checkpoint, langgraph-sdk\n",
      "Required-by: \n",
      "---\n",
      "Name: langchain-openai\n",
      "Version: 0.2.12\n",
      "Summary: An integration package connecting OpenAI and LangChain\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\CGM\\anaconda3\\Lib\\site-packages\n",
      "Requires: langchain-core, openai, tiktoken\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# pip install python-dotenv\n",
    "# pip install langgraph\n",
    "!pip show python-dotenv langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6744b3-959f-4f97-8fb9-f2367239b1f7",
   "metadata": {},
   "source": [
    "Nodes act like functions that can be called as needed. In our case Node 1 is our starting point and Node 2 is our finish point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd28001-6971-4272-abea-5035ca4439ee",
   "metadata": {},
   "source": [
    "## 1. Simplest Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b7bdf2-5da0-4d58-8ec9-7c8d19ed581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    return input_1 + \" Hi \"\n",
    "\n",
    "def function_2(input_2):\n",
    "    return input_2 + \"there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33fad35e-4224-4541-9fd4-0df4ce813863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Hi there'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "# Creates an instance of the Graph class and assigns it to the variable\n",
    "workflow1 = Graph()\n",
    "\n",
    "# Adds two nodes to the graph\n",
    "workflow1.add_node(\"node_1\", function_1)\n",
    "workflow1.add_node(\"node_2\", function_2)\n",
    "\n",
    "# Adds a directed edge from node_1 to node_2\n",
    "workflow1.add_edge('node_1', 'node_2')\n",
    "\n",
    "# Sets node_1 as the entry point of the workflow\n",
    "workflow1.set_entry_point(\"node_1\")\n",
    "# Sets node_2 as the finish point of the workflow\n",
    "workflow1.set_finish_point(\"node_2\")\n",
    "\n",
    "# Compiles the graph into an executable application or object\n",
    "app1 = workflow1.compile()\n",
    "\n",
    "# Executes the compiled workflow (app) with \"Hello\" as an input.\n",
    "app1.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29452184-5103-4da7-b6e6-afa56b897f6a",
   "metadata": {},
   "source": [
    "- `add_node(name, function)`: Adds a node to the graph. Each node represents a step in the workflow, and itâ€™s linked to a specific callable function (e.g., function_1 or function_2).\n",
    "\n",
    "- `add_edge(from_node, to_node)`: Establishes a directed connection between two nodes. This defines the execution order, ensuring to_node runs after from_node.\n",
    "\n",
    "- `set_entry_point(node)`: Marks the starting node of the workflow. Execution begins here.\n",
    "\n",
    "- `set_finish_point(node)`: Marks the final node of the workflow. Execution ends when this node completes.\n",
    "\n",
    "- `compile()`: Converts the graph into an executable format, preparing the workflow for running the defined sequence of tasks.\n",
    "- `invoke` method is typically used to trigger the workflow and pass any required input (in this case, the string \"Hello\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd2f14a-9f41-49c7-be74-d8e7eec3d294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'node_1':\n",
      "---\n",
      "Hello Hi \n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'node_2':\n",
      "---\n",
      "Hello Hi there\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = 'Hello'\n",
    "for output in app1.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945eb681-4d1c-4360-b928-acbe07f1c772",
   "metadata": {},
   "source": [
    "- `stream` method in the context of this code is used to <b>execute the workflow node by node and yield results progressively.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806856b-3d2e-45b8-a426-9003541c730c",
   "metadata": {},
   "source": [
    "## 2. Adding LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e624a9b-eb46-48ec-9284-030db68b69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Now you can access your environment variables using os.environ\n",
    "os.environ['OPENAI_API_KEY'] = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af8ac764-55fd-45d3-9391-1000864640b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-cbbf06f4-6bc0-4534-88e0-ea629dcfdf69-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Set the model as ChatOpenAI\n",
    "model = ChatOpenAI(temperature=0) \n",
    "\n",
    "#Call the model with a user message\n",
    "model.invoke('Hey there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8fe5e9b-9491-450c-b4e7-496b1b5bba0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke('Hey there').content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b34cc2-eb6a-4fbe-a103-f379b249cbd6",
   "metadata": {},
   "source": [
    "- `.content:` An attribute of the response object that holds the main output (e.g., generated text or result).\n",
    "- Use `invoke` when you want to <b>send data to the model, process it, and get a structured response back.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1432a9fb-35df-47db-8366-51fe87ec650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_3(input_3):\n",
    "    # Sends the input `input_1` to a model using the `invoke` method.\n",
    "    # Retrieves the model's response and returns the `content` attribute of the response.\n",
    "    response = model.invoke(input_3)\n",
    "    return response.content\n",
    "\n",
    "def function_4(input_4):\n",
    "    # Prepends the string \"Agent Says: \" to the input `input_2` and returns the resulting string.\n",
    "    return \"Agent Says: \" + input_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0190af20-7180-45ba-bd1c-c7f9ffa658c1",
   "metadata": {},
   "source": [
    "1. Define a Langchain graph\n",
    "2. Creates a new instance of a Langchain `Graph` object, which will represent the workflow structure.\n",
    "3. Adds a node named \"agent\" to the workflow, associated with `function_3`.\n",
    "4. This node will serve as the starting point and execute the logic defined in `function_3`.\n",
    "5. Adds another node named \"node_2\", associated with `function_4`.\n",
    "6. This represents the next step in the workflow after \"agent\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61bd1e83-bb03-4174-ad90-5e62efa4bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow2 = Graph()\n",
    "\n",
    "workflow2.add_node(\"agent\", function_3)\n",
    "\n",
    "workflow2.add_node(\"node_2\", function_4)\n",
    "workflow2.add_edge('agent', 'node_2')\n",
    "\n",
    "workflow2.set_entry_point(\"agent\")\n",
    "workflow2.set_finish_point(\"node_2\")\n",
    "\n",
    "app2 = workflow2.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb7a0a69-a451-46fe-ae2b-f6614ced13c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent Says: Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app2.invoke(\"Hey there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c3504a3-b289-4f02-bc0e-318ab3116fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent':\n",
      "---\n",
      "Hello! How can I assist you today?\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'node_2':\n",
      "---\n",
      "Agent Says: Hello! How can I assist you today?\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = 'Hey there'\n",
    "for output in app2.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c873b4db-ace4-47a2-a53b-2ec15c10d1ca",
   "metadata": {},
   "source": [
    "## 3. First functional Agent App - City Temperature\n",
    "### Step 1: Parse the city mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "793dd62f-4ac4-42b1-9577-1840c3a8edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_5(input_5):\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "        Nothing more, just the city name mentioned. Following is the user query: \" + input_5\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content\n",
    "\n",
    "def function_6(input_6):\n",
    "    return \"Agent Says: \" + input_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c5c31be-554f-4e72-b0a1-ef3ed1a3d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Langchain graph\n",
    "workflow3 = Graph()\n",
    "\n",
    "#calling node 1 as agent\n",
    "workflow3.add_node(\"agent\", function_5)\n",
    "workflow3.add_node(\"node_2\", function_6)\n",
    "\n",
    "workflow3.add_edge('agent', 'node_2')\n",
    "\n",
    "workflow3.set_entry_point(\"agent\")\n",
    "workflow3.set_finish_point(\"node_2\")\n",
    "\n",
    "app3 = workflow3.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d68884e-718b-4377-baea-7b3d3fd0ac6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent Says: Madrid'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app3.invoke(\"What's the temperature in Madrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7947cdc1-8ae1-4afa-bbfd-a87d08ca60c8",
   "metadata": {},
   "source": [
    "### Step 2: Adding a weather API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af1454fb-252e-46dd-9ec3-0a26456a7683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyowm\n",
      "Version: 3.3.0\n",
      "Summary: A Python wrapper around OpenWeatherMap web APIs\n",
      "Home-page: https://github.com/csparpa/pyowm\n",
      "Author: Claudio Sparpaglione\n",
      "Author-email: csparpa@gmail.com\n",
      "License: MIT\n",
      "Location: C:\\Users\\CGM\\anaconda3\\Lib\\site-packages\n",
      "Requires: geojson, PySocks, requests\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show pyowm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d005fa8a-76a3-46fe-b0cf-14c26d2825a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
    "load_dotenv()\n",
    "os.environ[\"OPENWEATHERMAP_API_KEY\"] = os.environ.get(\"OPENWEATHERMAP_API_KEY\")\n",
    "\n",
    "weather = OpenWeatherMapAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de8502ee-ccbb-405a-9367-a87722f206df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Seoul, the current weather is as follows:\n",
      "Detailed status: scattered clouds\n",
      "Wind speed: 2.93 m/s, direction: 306Â°\n",
      "Humidity: 46%\n",
      "Temperature: \n",
      "  - Current: -1.69Â°C\n",
      "  - High: -1.69Â°C\n",
      "  - Low: -1.69Â°C\n",
      "  - Feels like: -5.46Â°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 27%\n"
     ]
    }
   ],
   "source": [
    "weather_data = weather.run(\"Seoul\")\n",
    "print(weather_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "633137a0-b838-4927-b46e-a430eefd5320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_7(input_7):\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "        Nothing more, just the city name mentioned. Following is the user query: \" + input_7\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content\n",
    "\n",
    "def function_8(input_8):\n",
    "    weather_data = weather.run(input_8)\n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d122136-bd7a-4a89-a918-bb4fa5d5797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow4 = Graph()\n",
    "\n",
    "#calling node 1 as agent\n",
    "workflow4.add_node(\"agent\", function_7)\n",
    "workflow4.add_node(\"tool\", function_8)\n",
    "\n",
    "workflow4.add_edge('agent', 'tool')\n",
    "\n",
    "workflow4.set_entry_point(\"agent\")\n",
    "workflow4.set_finish_point(\"tool\")\n",
    "\n",
    "app4 = workflow4.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "144a4d8b-d948-47a4-b2e6-2ddc9e8295c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Helsinki, the current weather is as follows:\\nDetailed status: mist\\nWind speed: 4.02 m/s, direction: 74Â°\\nHumidity: 97%\\nTemperature: \\n  - Current: 1.44Â°C\\n  - High: 2.65Â°C\\n  - Low: 0.25Â°C\\n  - Feels like: -2.53Â°C\\nRain: {}\\nHeat index: None\\nCloud cover: 75%'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app4.invoke(\"What's the temperature in Helsinki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46509b8f-5b9f-4d40-be75-c126006cc4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent':\n",
      "---\n",
      "Stockholm\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'tool':\n",
      "---\n",
      "In Stockholm, the current weather is as follows:\n",
      "Detailed status: mist\n",
      "Wind speed: 3.6 m/s, direction: 250Â°\n",
      "Humidity: 98%\n",
      "Temperature: \n",
      "  - Current: 5.04Â°C\n",
      "  - High: 5.59Â°C\n",
      "  - Low: 4.86Â°C\n",
      "  - Feels like: 2.15Â°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 75%\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = \"What's the temperature in Stockholm\"\n",
    "for output in app4.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd91ac6-710f-4787-bd78-d7d473eecf11",
   "metadata": {},
   "source": [
    "### Step 3 Adding another LLM Call to filter results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7387064d-054f-448d-b0f9-724112fc9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_11(input_3):\n",
    "    complete_query = \"Your task is to provide info concisely based on the user query. Following is the user query: \" + \"user input\"\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "097bde0f-da03-4b1a-a173-4a47ed98875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign AgentState as an empty dict\n",
    "AgentState = {}\n",
    "\n",
    "# messages key will be assigned as an empty array. We will append new messages as we pass along nodes. \n",
    "AgentState[\"messages\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39c19c3b-d9b5-4bea-a7db-0c1608184e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': []}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61bf8f1e-8eaa-4038-8577-3a36a32b1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_9(state):\n",
    "    messages = state['messages']\n",
    "    user_input = messages[-1]\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "                    Nothing more, just the city name mentioned. Following is the user query: \" + user_input\n",
    "    response = model.invoke(complete_query)\n",
    "    state['messages'].append(response.content) # appending AIMessage response to the AgentState\n",
    "    return state\n",
    "\n",
    "def function_10(state):\n",
    "    messages = state['messages']\n",
    "    agent_response = messages[-1]\n",
    "    weather = OpenWeatherMapAPIWrapper()\n",
    "    weather_data = weather.run(agent_response)\n",
    "    state['messages'].append(weather_data)\n",
    "    return state\n",
    "\n",
    "def function_11(state):\n",
    "    messages = state['messages']\n",
    "    user_input = messages[0]\n",
    "    available_info = messages[-1]\n",
    "    agent2_query = \"Your task is to provide concisely the current temperature based on the available information. \\\n",
    "                    Do not include any other details. Following is the user query: \" + user_input + \\\n",
    "                    \" Available information: \" + available_info\n",
    "    response = model.invoke(agent2_query)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6445d60f-bf2c-4560-be78-cf31993a0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow5 = Graph()\n",
    "\n",
    "workflow5.add_node(\"agent\", function_9)\n",
    "workflow5.add_node(\"tool\", function_10)\n",
    "workflow5.add_node(\"responder\", function_11)\n",
    "\n",
    "workflow5.add_edge('agent', 'tool')\n",
    "workflow5.add_edge('tool', 'responder')\n",
    "\n",
    "workflow5.set_entry_point(\"agent\")\n",
    "workflow5.set_finish_point(\"responder\")\n",
    "\n",
    "app5 = workflow5.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f405e399-66a6-402e-8534-c8532ca333e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in Singapore is 29.15Â°C.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"what is the temperature in Singapore\"]}\n",
    "app5.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bac0e3db-32e2-488f-aaff-391f4243c0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent':\n",
      "---\n",
      "{'messages': ['what is the temperature in Singapore', 'Singapore']}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'tool':\n",
      "---\n",
      "{'messages': ['what is the temperature in Singapore', 'Singapore', 'In Singapore, the current weather is as follows:\\nDetailed status: broken clouds\\nWind speed: 6.69 m/s, direction: 20Â°\\nHumidity: 68%\\nTemperature: \\n  - Current: 29.15Â°C\\n  - High: 29.97Â°C\\n  - Low: 27.95Â°C\\n  - Feels like: 32.68Â°C\\nRain: {}\\nHeat index: None\\nCloud cover: 75%']}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'responder':\n",
      "---\n",
      "The current temperature in Singapore is 29.15Â°C.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = {\"messages\": [\"what is the temperature in Singapore\"]}\n",
    "for output in app5.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "351dc7ca-b920-4f43-beb6-5774b80d4acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in Istanbul is 8.68Â°C.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2 = {\"messages\": [\"How are you today?\"]}\n",
    "app5.invoke(input2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a22264b-1b9e-41eb-bea8-7b640f619d43",
   "metadata": {},
   "source": [
    "### Step 4 AgentState\n",
    "Manages an agent's state by storing <b>conversation history</b> and <b>workflow context</b> in a `structured dictionary`, enabling <b>dynamic decision-making</b> and <b>coherent interactions</b> in chatbots and automated systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa45ea-b58e-4070-ae2c-4fa990fbb2fc",
   "metadata": {},
   "source": [
    "1. `TypedDict`: Used to create structured dictionary-like classes with predefined keys and types.\n",
    "2. `Annotated`: Allows attaching metadata or context to a type.\n",
    "3. `Sequence`: Represents a generic ordered collection, such as a list or tuple.\n",
    "4. `operator`: Provides functional equivalents of built-in operators, like addition, subtraction, etc.\n",
    "5. `BaseMessage`: Represents a fundamental message structure used in the LangChain framework (It is likely used for storing or processing individual messages in a workflow or conversation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddc6d3bd-dd62-4501-8119-127dffe10e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f527d7-d3dc-4427-a66e-c1fb7033b01f",
   "metadata": {},
   "source": [
    "1. `AgentState` define a structured dictionary class called AgentState\n",
    "2. It inherits from `TypedDict`, which allows specifying fixed keys and their associated types.\n",
    "3. Define the key `messages`:\n",
    "    - It is a sequence of `BaseMessage` objects (e.g., a list or tuple).\n",
    "    - The `Annotated` type wraps the sequence type to include additional metadata.\n",
    "    - `operator.add` is used as metadata, indicating that this sequence can be combined using addition (concatenation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e840cc52-baa3-445c-aeb6-ec702ce4d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b6f30-e755-4b66-89de-b59b8c5b0c94",
   "metadata": {},
   "source": [
    "1. `convert_to_openai_function` to make tools compatible with OpenAI's API.\n",
    "2. `OpenWeatherMapQueryRun` a prebuilt tool to fetch weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "368b114b-9d50-498c-9c6c-edc7ab86f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_community.tools.openweathermap import OpenWeatherMapQueryRun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89e4443-ecb9-41f5-9a99-f28d759be5ab",
   "metadata": {},
   "source": [
    "1. `tools` initializes a list of tools, specifically the OpenWeatherMapQueryRun tool.\n",
    "2. The tool interacts with the OpenWeatherMap API to fetch weather data, such as temperature, humidity, etc.\n",
    "3. Additional tools can be added to this list as needed.\n",
    "4. `model` initializes an instance of the ChatOpenAI model with specific parameters:\n",
    "   - `streaming=True`: Enables streaming responses, allowing partial outputs to\n",
    "6. `functions` converts each tool in the `tools` list into an OpenAI-compatible function.\n",
    "   - `tools`: A list of tool instances, such as APIs or custom functions.\n",
    "   - `convert_to_openai_function(t)`: A utility that wraps each tool to make it usable with OpenAI's function-calling API.\n",
    "7. `model` binds the initialized tools (e.g., OpenWeatherMapQueryRun) to the ChatOpenAI model.\n",
    "   - This enables the model to invoke these tools during its operation, enhancing its ability to fetch real-time weather data.\n",
    "   - The model can now use the tool(s) dynamically based on the conversation or function call requirements.\n",
    "   - `bind_tools` links external tools (e.g., APIs or functions) to the OpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f96517ae-df27-4a39-aaf6-a6d42142f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [OpenWeatherMapQueryRun()]\n",
    "model = ChatOpenAI(temperature=0, streaming=True)\n",
    "functions = [convert_to_openai_function(t) for t in tools]\n",
    "model = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fa4e99-6db4-487b-94e8-ed10f9d829d3",
   "metadata": {},
   "source": [
    "1. This function processes the given `state`, which is expected to be a dictionary.\n",
    "2. The state contains a key `messages` that holds a sequence of conversation messages.\n",
    "3. `messages` extracts the messages from the state. These represent the conversation context so far.\n",
    "4. `response` sends the messages to the `model` using its `invoke` method.\n",
    "   - The `model` processes the messages (e.g., user input or context) and generates a response.\n",
    "   - The response is likely a new message generated by the AI model.\n",
    "5. Returns a new state with the updated `messages`.\n",
    "   - The new state contains only the response in its `messages` key.\n",
    "   - This could be useful for resetting or isolating responses in a workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ad58ab4-3492-4914-8caa-8bcc3a8b3c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_12(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eec880-802d-4a9e-a2f9-55c14c0bfb1c",
   "metadata": {},
   "source": [
    "1. `ToolNode` is used to integrate and execute tools within a LangGraph workflow.\n",
    "   - It allows defining nodes in the graph that can perform specific tasks using external tools.\n",
    "2. `FunctionMessage` represents a message generated by a tool or function in a workflow.\n",
    "   - It can store the content of the message and metadata like the function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45499f0-7200-4813-a402-cc6e34e26388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4beea9-cddd-4ba3-8ad0-1426de01739d",
   "metadata": {},
   "source": [
    "1. Creates a new ToolNode instance and assigns it to the variable \"tool_node\".\n",
    "   - `ToolNode`: A class from LangGraph used to define nodes in a workflow that execute tools.\n",
    "   - `tools`: A list or collection of tool instances (e.g., API integrations or custom functions).\n",
    "   - These tools are registered with the ToolNode, enabling the node to invoke them during execution.\n",
    "2. \"function_13\" processes the current agent state to extract the latest message,\n",
    "   - invokes a tool based on the message's details, and updates the state with the tool's response.\n",
    "3. `messages` extracts the list of messages from the current state.\n",
    "   - `messages` contains the conversation history or workflow messages.\n",
    "4. \"last_message\" retrieves the last message in the conversation.\n",
    "   - This is assumed to contain the query or function call details that need to be sent to the tool.\n",
    "5. \"parsed_tool_input\" parses the arguments from the last message's `function_call`.\n",
    "   - The arguments are expected to be in JSON format, so they are decoded using `json.loads`.\n",
    "6. \"action\" constructs a `ToolInvocation` object with the tool name and the specific input extracted from the message.\n",
    "   - `tool`: The name of the tool to be invoked, extracted from the function call metadata.\n",
    "   - `tool_input`: The actual input for the tool, retrieved from the parsed arguments.\n",
    "7. \"response\" calls the tool using the ToolNode (`tool_node`) and retrieves its response.\n",
    "8. \"function_message\" creates a new FunctionMessage to encapsulate the tool's response.\n",
    "   - `content`: The response from the tool, converted to a string.\n",
    "   - `name`: The name of the tool that generated the response.\n",
    "9. Returns the updated state containing the new message (the tool's response).\n",
    "   - This ensures the workflow can continue with the updated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff0877c2-a838-452d-9750-7465b3a06ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node = ToolNode(tools)\n",
    "\n",
    "def function_13(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1] # this has the query we need to send to the tool provided by the agent\n",
    "    parsed_tool_input = json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"])\n",
    "\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=parsed_tool_input['__arg1'],\n",
    "    )\n",
    "    response = tool_node.invoke(action)\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e0d635-c99b-4b0f-ad65-650fe17c28a9",
   "metadata": {},
   "source": [
    "1. \"where_to_go\" determines the next step in a workflow based on the state of the conversation.\n",
    "   - Specifically, it checks if the last message contains a \"function_call\" instruction.\n",
    "2. \"messages\" extracts the list of messages from the current state.\n",
    "   - These messages represent the conversation history or the workflow context.\n",
    "3. \"last_message\" retrieves the last message in the conversation.\n",
    "   - This message contains the latest user query or system response.\n",
    "4. The \"if function\" checks if the last message contains a \"function_call\" in its additional metadata (kwargs).\n",
    "   - This indicates that a function/tool needs to be executed based on the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b009f53-217f-4209-b55a-621d6ee8f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def where_to_go(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    if \"function_call\" in last_message.additional_kwargs:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a94b5-a2f5-4601-9d2e-fbd108a5d243",
   "metadata": {},
   "source": [
    "1. `StateGraph`: A class used to define and manage workflows or state-based transitions in LangGraph.\n",
    "   - It provides a way to build graphs where each node represents a state or function in the workflow.\n",
    "2. `END`: A special constant used to denote the end point of the graph or workflow. It signals where the process should terminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "740db995-b9c1-4b8b-96e8-2dc9ab3e8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b992b934-a266-400d-8489-cc4ff4216107",
   "metadata": {},
   "source": [
    "1. \"workflow6\" initializes a new workflow using the StateGraph class.\n",
    "2. `StateGraph`: A graph-based framework for defining workflows or processes, where each node represents a state or function.\n",
    "3. `AgentState`: The structured dictionary that represents the state of the agent, including its messages and other relevant data.\n",
    "4. <b>`add_conditional_edges`</b> requires the following info below.\n",
    "   - First, we define the start node. We use `agent`.\n",
    "   - This means these are the edges taken after the `agent` node is called.\n",
    "   - Next, we pass in the function that will determine which node is called next, in our case where_to_go()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b84c1daa-06e9-462f-9e03-cfbdc03ac698",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow6 = StateGraph(AgentState)\n",
    "\n",
    "workflow6.add_node(\"agent\", function_12)\n",
    "workflow6.add_node(\"tool\", function_13)\n",
    "\n",
    "workflow6.add_conditional_edges(\"agent\", where_to_go,{  # Based on the return from where_to_go\n",
    "                                                        # If return is \"continue\" then we call the tool node.\n",
    "                                                        \"continue\": \"tool\",\n",
    "                                                        # Otherwise we finish. END is a special node marking that the graph should finish.\n",
    "                                                        \"end\": END\n",
    "                                                    }\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that if `tool` is called, then it has to call the 'agent' next. \n",
    "workflow6.add_edge('tool', 'agent')\n",
    "\n",
    "# Basically, agent node has the option to call a tool node based on a condition, \n",
    "# whereas tool node must call the agent in all cases based on this setup.\n",
    "workflow6.set_entry_point(\"agent\")\n",
    "\n",
    "app6 = workflow6.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a5cbd-7a71-4bc4-b89a-7f1ad30ae9e5",
   "metadata": {},
   "source": [
    "1. Imports the `HumanMessage` class from the LangChain Core module.\n",
    "   - `HumanMessage`: Represents a message sent by a human user in a conversation or workflow.\n",
    "   - This is part of the LangChain framework's messaging system, which structures and manages interactions between users, agents, and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f06d9ea-b039-42ad-b178-5b162069d144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the temperature in Seoul', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_Ze6HNUEmYSRKKGKt5S2qlyks', 'function': {'arguments': '{\"location\":\"Seoul\"}', 'name': 'open_weather_map'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-8a5088a2-393c-4d73-9130-897d263a1aac-0', tool_calls=[{'name': 'open_weather_map', 'args': {'location': 'Seoul'}, 'id': 'call_Ze6HNUEmYSRKKGKt5S2qlyks', 'type': 'tool_call'}])]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in Seoul\")]}\n",
    "app6.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "4b9893d7-58be-4033-b70e-7a7079298bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent':\n",
      "---\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_iQfug9LwSS86TYmX0GfxbWfR', 'function': {'arguments': '{\"location\":\"Seoul\"}', 'name': 'open_weather_map'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-8ad458b5-156d-45c6-8f3f-66be6a63e1e7-0', tool_calls=[{'name': 'open_weather_map', 'args': {'location': 'Seoul'}, 'id': 'call_iQfug9LwSS86TYmX0GfxbWfR', 'type': 'tool_call'}])]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in Seoul\")]}\n",
    "for output in app6.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3be69-ce0b-40a6-ad6c-23742f162f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
